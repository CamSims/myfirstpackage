---
title: "Project 3: myfirstpackage Tutorial"
author: "Cameron Sims"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{myfirstpackage Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Introduction

This package was designed for my UW STAT 302 course's Project 3. It contains
four methods for performing statistical inference/prediction:
\begin{itemize}
\item Linear Regression
\item T-Test
\item K-Nearest Neighbors with Cross-Validation
\item Random Forests with Cross Validation
\end{itemize}

```{r, include = FALSE, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

To install this package through Github, use

```{r, eval = FALSE}
devtools::install_github("CamSims/myfirstpackage")
```

To begin, we load the package.

```{r setup}
library(myfirstpackage)
library("ggplot2")
```

# my_t.test Tutorial

This section will demonstrate how to use the my_t.test() function.

```{r}
# Use the lifeExp data from my_gapminder
dat <- my_gapminder$lifeExp

# Demonstrate a test of the hypothesis H0: mu = 60, Ha: mu != 60
my_t.test(x = dat, alternative = "two.sided", mu = 60)
```
At a cutoff of $\alpha = 0.05$, the p-value of $0.0932$ means the null
hypothesis can not be rejected. In other words, $60$ is a valid value for $\mu$.

```{r}
# Demonstrate a test of the hypothesis H0: mu = 60, Ha: mu < 60
my_t.test(x = dat, alternative = "less", mu = 60)
```

Using the $\alpha = 0.05$ cutoff, the p-value of $0.0466$ means the null
hypothesis can be rejected. In other words, the data suggests $\mu$ is less than
$60$.

```{r}
# Demonstrate a test of the hypothesis H0: mu = 60, Ha: mu > 60
my_t.test(x = dat, alternative = "greater", mu = 60)
```

Using the $\alpha = 0.05$ cutoff, the p-value of $.9533$ means the null
hypothesis can not be rejected. The data suggests $mu = 60$ is a valid value.


# my_lm Tutorial

This section will demonstrate how to use the my_lm() function.

```{r}
# Performing linear regression on lifeExp using gdpPercap and continent
model <- my_lm(lifeExp ~ gdpPercap + continent, data = my_gapminder)

# Show the model
model
```

The gdpPercap coefficient indicates that a unit increase results in an increase
of roughly .0004 in life expectancy. Let $\beta_{1}$ be the coefficient. The
hypothesis test associated with this is then $H_{0}: \beta_{1} = 0$,
$H_{a}:\beta_{1} \neq 0$. The associated p-value is very close to 0, so a
cut-off of $\alpha = 0.05$ results in rejection of the null hypothesis. In other
words, the data suggests $\beta_{1}$ is not $0$.

```{r}
# Calculate fitted values
fitted <- model$Estimate[1] + model$Estimate[2] * my_gapminder$gdpPercap +
  model$Estimate[2] * (my_gapminder$continent == "Americas") +
  model$Estimate[3] * (my_gapminder$continent == "Asia") + 
  model$Estimate[4] * (my_gapminder$continent == "Europe") +
  model$Estimate[5] * (my_gapminder$continent == "Oceania")
  
# Plot Actual vs Fitted Values
ggplot(mapping = aes(x = fitted, y = my_gapminder$lifeExp)) + geom_line() +
  labs(title = "Actual Vs Fitted Values") +
  xlab("Fitted Value") +
  ylab("Actual Value")
```

From the plot, it appears the fitted values don't match the actual values very
well. This means the model fit isn't very good.


# my_knn_cv Tutorial

```{r}
# Remove NA values and select training columns
train_dat <- na.omit(my_penguins)[c("bill_length_mm", "bill_depth_mm",
                  "flipper_length_mm", "body_mass_g")]

# Omit NA values and select classifying column
correct <- data.frame("species" = na.omit(my_penguins)$species)

# Arrays to hold results
training_errors <- c()
cv_misclass <- c()

# Perform k nearest_neighbors
for (k in 1:10) {
  results <- my_knn_cv(train_dat, correct, k_nn = k, k_cv = 5)
  cv_misclass[k] <- results$cv_err
  training_errors[k] <- mean(results$class != correct$species)
}

# Find model with least training error
least_train <- which.min(training_errors)

# Find model with least cv misclassification rate
least_cv_misclass <- which.min(cv_misclass)
```

The model to choose based on the training misclassification rates is the one
using `r least_train` neighbors. The model to choose based on CV
misclassification rates is the one that uses `r least_cv_misclass` neighbors. In
practice, you would choose the model that uses `r least_cv_misclass` neighbors.
CV (cross-validation) means the method is splitting the data into a given number
of folds, in this case 5, then going one fold at a time. It uses the current
fold as its testing data and all other folds training data. This allows the
method to evaluate the performance of the covariates on data that was not used
to train it. The model chosen to use in practice has the lowest CV
misclassification rate, which means it can better predict new data that is added
to it.
